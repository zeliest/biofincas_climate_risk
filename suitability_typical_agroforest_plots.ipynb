{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6c55c5-aad9-4821-8e4c-f2ae1bae514f",
   "metadata": {},
   "source": [
    "## Agroforestry Suitability Workflow\n",
    "\n",
    "This notebook evaluates the **climate suitability** of agroforestry systems **at two scales**:\n",
    "\n",
    "1. **Plot-specific analysis**  \n",
    "   - Focused on a given plot location (latitude, longitude, elevation).  \n",
    "   - Shows how suitability for each species may change **at this exact site** under future climate scenarios.\n",
    "\n",
    "2. **Regional analysis**  \n",
    "   - Looks at the wider production landscape around the plot.  \n",
    "   - Maps current and projected suitability across the region.  \n",
    "   - Puts the plot‚Äôs results in the context of where the crop or species is most likely to thrive.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Overview\n",
    "\n",
    "1. **Start from a plot**  \n",
    "   - Define the plot location and record the current agroforestry system (species, density, shading, yield).\n",
    "\n",
    "2. **Gather species occurrence data**  \n",
    "   - Search GBIF and/or local datasets for occurrences in a **wider training region**.  \n",
    "   - Clean, filter, and spatially thin to reduce bias.  \n",
    "   - Optionally remove isolated or wild points if focusing on cultivated systems.\n",
    "\n",
    "3. **Prepare environmental predictors**  \n",
    "   - Load current climate data (e.g., TerraClimate).  \n",
    "\n",
    "4. **Generate background (absence) points**  \n",
    "   - Sample across the training region, restricted to land.  \n",
    "   - Ensure coverage across environmental gradients (e.g., lowlands, highlands).\n",
    "\n",
    "5. **Train the species distribution model (SDM)**  \n",
    "   - Use a Random Forest classifier with presence/absence data.  \n",
    "   - Evaluate performance (AUC, sensitivity, specificity).  \n",
    "   - Determine a probability threshold for suitability.\n",
    "\n",
    "6. **Predict suitability**  \n",
    "   - For **current** climate conditions.  \n",
    "   - For **future** climate scenarios.\n",
    "\n",
    "7. **Analyse results**  \n",
    "   - **Plot scale:** Extract suitability values for the exact plot location and compare now vs future.  \n",
    "   - **Regional scale:** Map suitability categories (e.g., suitable both, newly suitable, no longer suitable) across the training region.\n",
    "\n",
    "---\n",
    "\n",
    "**Goal:**  \n",
    "Provide decision‚Äëmakers with:\n",
    "- **Actionable insights for their plot** (current system resilience, alternative scenarios).  \n",
    "- **Regional context** to understand how local changes fit into broader production trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1a5d69-8574-4207-9104-d0e546bd313a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annona muricata\n",
      "Artocarpus altilis\n",
      "Byrsonima crassifolia\n",
      "Cajanus cajan\n",
      "Carica papaya\n",
      "Cedrela odorata\n",
      "Citrus aurantium\n",
      "Citrus sinensis\n",
      "Coffea arabica\n",
      "Cordia alliodora\n",
      "Erythrina poeppigiana\n",
      "Gliricidia sepium\n",
      "Inga edulis\n",
      "Inga jinicuil\n",
      "Inga vera\n",
      "Macadamia integrifolia\n",
      "Mangifera indica\n",
      "Musa paradisiaca\n",
      "Persea americana\n",
      "Pouteria sapota\n",
      "Psidium guajava\n",
      "Spondias mombin\n",
      "Swietenia macrophylla\n",
      "Theobroma cacao\n",
      "\n",
      "Total unique species: 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config import DATA_DIR\n",
    "\n",
    "# Path to your cleaned observation file\n",
    "obs_path = DATA_DIR / \"species_location/species_occurrences_combined_cleaned.csv\"\n",
    "\n",
    "# Read file\n",
    "df_obs = pd.read_csv(obs_path)\n",
    "\n",
    "# Normalise column names\n",
    "rename_map = {\n",
    "    \"species\": \"species_query\", \"scientificName\": \"species_query\",\n",
    "    \"Longitude\": \"decimalLongitude\", \"lon\": \"decimalLongitude\", \"lng\": \"decimalLongitude\",\n",
    "    \"Latitude\": \"decimalLatitude\",   \"lat\": \"decimalLatitude\",\n",
    "}\n",
    "for k, v in rename_map.items():\n",
    "    if k in df_obs.columns and v not in df_obs.columns:\n",
    "        df_obs = df_obs.rename(columns={k: v})\n",
    "\n",
    "# Drop rows with no species info\n",
    "df_obs = df_obs.dropna(subset=[\"species_query\"])\n",
    "\n",
    "# Get unique species\n",
    "unique_species = sorted(df_obs[\"species_query\"].unique())\n",
    "\n",
    "# Show as list\n",
    "for sp in unique_species:\n",
    "    print(sp)\n",
    "\n",
    "print(f\"\\nTotal unique species: {len(unique_species)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ceb132-cd8a-4094-a94f-438549ddc51c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annona muricata: 914 ‚Üí 457 after cleaning\n",
      "Artocarpus altilis: 1108 ‚Üí 558 after cleaning\n",
      "Byrsonima crassifolia: 2330 ‚Üí 1340 after cleaning\n",
      "Cajanus cajan: 469 ‚Üí 255 after cleaning\n",
      "Carica papaya: 2610 ‚Üí 1399 after cleaning\n",
      "Cedrela odorata: 1408 ‚Üí 772 after cleaning\n",
      "Citrus aurantium: 256 ‚Üí 103 after cleaning\n",
      "Citrus sinensis: 362 ‚Üí 146 after cleaning\n",
      "Coffea arabica: 3561 ‚Üí 1725 after cleaning\n",
      "Cordia alliodora: 1272 ‚Üí 615 after cleaning\n",
      "Erythrina poeppigiana: 133 ‚Üí 60 after cleaning\n",
      "Gliricidia sepium: 1914 ‚Üí 1027 after cleaning\n",
      "Inga edulis: 1570 ‚Üí 866 after cleaning\n",
      "Inga jinicuil: 194 ‚Üí 88 after cleaning\n",
      "Inga vera: 1989 ‚Üí 1155 after cleaning\n",
      "Macadamia integrifolia: 10 ‚Üí 1 after cleaning\n",
      "Mangifera indica: 2018 ‚Üí 1068 after cleaning\n",
      "Musa paradisiaca: 487 ‚Üí 224 after cleaning\n",
      "Persea americana: 1878 ‚Üí 1072 after cleaning\n",
      "Pouteria sapota: 476 ‚Üí 225 after cleaning\n",
      "Psidium guajava: 3034 ‚Üí 1693 after cleaning\n",
      "Spondias mombin: 820 ‚Üí 373 after cleaning\n",
      "Swietenia macrophylla: 550 ‚Üí 230 after cleaning\n",
      "Theobroma cacao: 944 ‚Üí 501 after cleaning\n",
      "‚úÖ Cleaned dataset: 15953 records\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_occurrences(df, max_isolate_distance_deg=0.5, thinning_deg=2.5/60):\n",
    "    \"\"\"\n",
    "    Clean occurrence points per species by:\n",
    "    1. Removing isolated points farther than `max_isolate_distance_deg` from all others.\n",
    "    2. Applying spatial thinning so that only 1 point is kept per grid cell of size `thinning_deg`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: species (or species_query), decimalLongitude, decimalLatitude.\n",
    "    max_isolate_distance_deg : float\n",
    "        Maximum allowed distance (in degrees) for a point to have a neighbour. \n",
    "        Points farther than this from all others are removed.\n",
    "    thinning_deg : float\n",
    "        Grid cell size for thinning, in degrees. Default 2.5 arc‚Äëmin (~0.0416667¬∞).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned DataFrame of occurrences.\n",
    "    \"\"\"\n",
    "\n",
    "    def remove_isolated_points(species_df):\n",
    "        coords = species_df[[\"decimalLongitude\", \"decimalLatitude\"]].to_numpy()\n",
    "        keep_idx = []\n",
    "        for i, c in enumerate(coords):\n",
    "            dists = np.sqrt((coords[:,0] - c[0])**2 + (coords[:,1] - c[1])**2)\n",
    "            dists[i] = np.inf  # ignore self\n",
    "            if np.any(dists < max_isolate_distance_deg):\n",
    "                keep_idx.append(i)\n",
    "        return species_df.iloc[keep_idx]\n",
    "\n",
    "    def thin_points(species_df):\n",
    "        # Assign grid cell indices\n",
    "        grid_x = np.floor(species_df[\"decimalLongitude\"] / thinning_deg)\n",
    "        grid_y = np.floor(species_df[\"decimalLatitude\"] / thinning_deg)\n",
    "        species_df = species_df.assign(grid_x=grid_x, grid_y=grid_y)\n",
    "        # Randomly sample 1 point per grid cell\n",
    "        thinned = species_df.groupby([\"grid_x\", \"grid_y\"], group_keys=False).apply(\n",
    "            lambda g: g.sample(1, random_state=42)\n",
    "        )\n",
    "        return thinned.drop(columns=[\"grid_x\", \"grid_y\"])\n",
    "\n",
    "    cleaned_species_list = []\n",
    "    for species, group in df.groupby(\"species_query\" if \"species_query\" in df.columns else \"species\"):\n",
    "        # 1Ô∏è‚É£ Remove extreme isolates\n",
    "        non_isolated = remove_isolated_points(group)\n",
    "        # 2Ô∏è‚É£ Apply spatial thinning\n",
    "        thinned = thin_points(non_isolated)\n",
    "        print(f\"{species}: {len(group)} ‚Üí {len(thinned)} after cleaning\")\n",
    "        cleaned_species_list.append(thinned)\n",
    "\n",
    "    return pd.concat(cleaned_species_list, ignore_index=True)\n",
    "\n",
    "\n",
    "# --- Example use ---\n",
    "df = clean_occurrences(\n",
    "    df_obs,\n",
    "    max_isolate_distance_deg=0.1,   # ~11 km\n",
    "    thinning_deg=2.5/60             # 2.5 arc-min (~4.6 km)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Cleaned dataset: {len(df)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42aeb161-4304-4003-8f36-ef477c6a18cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.682167</td>\n",
       "      <td>24.415475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.423805</td>\n",
       "      <td>23.193857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.429925</td>\n",
       "      <td>23.232595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.423988</td>\n",
       "      <td>23.260112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.443430</td>\n",
       "      <td>23.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15948</th>\n",
       "      <td>Theobroma cacao</td>\n",
       "      <td>-60.906423</td>\n",
       "      <td>14.601630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15949</th>\n",
       "      <td>Theobroma cacao</td>\n",
       "      <td>-60.859124</td>\n",
       "      <td>14.513195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15950</th>\n",
       "      <td>Theobroma cacao</td>\n",
       "      <td>-60.585957</td>\n",
       "      <td>11.254710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15951</th>\n",
       "      <td>Theobroma cacao</td>\n",
       "      <td>-60.549420</td>\n",
       "      <td>11.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15952</th>\n",
       "      <td>Theobroma cacao</td>\n",
       "      <td>-60.535130</td>\n",
       "      <td>11.325495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15953 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               species         lon        lat\n",
       "0      Annona muricata -106.682167  24.415475\n",
       "1      Annona muricata -106.423805  23.193857\n",
       "2      Annona muricata -106.429925  23.232595\n",
       "3      Annona muricata -106.423988  23.260112\n",
       "4      Annona muricata -106.443430  23.299000\n",
       "...                ...         ...        ...\n",
       "15948  Theobroma cacao  -60.906423  14.601630\n",
       "15949  Theobroma cacao  -60.859124  14.513195\n",
       "15950  Theobroma cacao  -60.585957  11.254710\n",
       "15951  Theobroma cacao  -60.549420  11.316900\n",
       "15952  Theobroma cacao  -60.535130  11.325495\n",
       "\n",
       "[15953 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occ_all = df.rename(columns={\"species_query\":\"species\",\n",
    "    \"decimalLongitude\": \"lon\",\n",
    "    \"decimalLatitude\": \"lat\",\n",
    "})[[\"species\", \"lon\", \"lat\"]].dropna()\n",
    "occ_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4226e-7a93-4e80-b754-124363edcf4c",
   "metadata": {},
   "source": [
    "In the following cell, we force some observation points from the agrosystems that we generated. As we need to make sure that those are suitable today.\n",
    "those would ideally correspond to true observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d40243-1c73-4e60-87df-fc85990ac1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final training set: 18395 points across 31 species\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.682167</td>\n",
       "      <td>24.415475</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.423805</td>\n",
       "      <td>23.193857</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.429925</td>\n",
       "      <td>23.232595</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.423988</td>\n",
       "      <td>23.260112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annona muricata</td>\n",
       "      <td>-106.443430</td>\n",
       "      <td>23.299000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           species         lon        lat source\n",
       "0  Annona muricata -106.682167  24.415475    NaN\n",
       "1  Annona muricata -106.423805  23.193857    NaN\n",
       "2  Annona muricata -106.429925  23.232595    NaN\n",
       "3  Annona muricata -106.423988  23.260112    NaN\n",
       "4  Annona muricata -106.443430  23.299000    NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ====================================================\n",
    "# Expert occurrence points (from Excel + manual input)\n",
    "# ====================================================\n",
    "data_dir = Path(\"agroforestry_systems\")\n",
    "excel_files = sorted(data_dir.glob(\"*.xlsx\"))\n",
    "\n",
    "# --- Manual expert cacao points ---\n",
    "expert_cacao = pd.DataFrame({\n",
    "    \"species\": [\"Theobroma cacao\"] * 3,\n",
    "    \"lon\": [-71.125, -71.228, -71.606],\n",
    "    \"lat\": [19.505, 19.602, 19.337],\n",
    "    \"source\": \"manual\"\n",
    "})\n",
    "\n",
    "# --- Manual expert coffee points ---\n",
    "expert_coffee = pd.DataFrame({\n",
    "    \"species\": [\"Coffea arabica\"] * 4,\n",
    "    \"lon\": [-71.243, -71.778, -70.981, -70.960],\n",
    "    \"lat\": [19.271, 19.404, 19.237, 19.670],\n",
    "    \"source\": \"manual\"\n",
    "})\n",
    "\n",
    "# --- Collect expert species/locations from Excel files ---\n",
    "expert_from_excel = []\n",
    "for file_path in excel_files:\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=\"present\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Standardise species names\n",
    "    if \"Scientific name\" in df.columns:\n",
    "        sp_col = \"Scientific name\"\n",
    "    else:\n",
    "        sp_col = \"Species\"\n",
    "\n",
    "    tmp = df.loc[df[sp_col].notna(), [sp_col, \"Longitude\", \"Latitude\"]].rename(\n",
    "        columns={sp_col: \"species\", \"Longitude\": \"lon\", \"Latitude\": \"lat\"}\n",
    "    )\n",
    "    tmp[\"source\"] = f\"excel:{file_path.stem}\"\n",
    "    expert_from_excel.append(tmp)\n",
    "\n",
    "# Merge everything\n",
    "expert_occ = pd.concat([expert_cacao, expert_coffee] + expert_from_excel, ignore_index=True)\n",
    "\n",
    "# ====================================================\n",
    "# Jiggle generator (species-specific)\n",
    "# ====================================================\n",
    "def generate_jiggle_points(base_points, n_per_point=10, jitter_deg_default=0.1):\n",
    "    jiggle_list = []\n",
    "    for _, row in base_points.iterrows():\n",
    "        # Coffee/cacao: altitude sensitive, keep tighter jitter\n",
    "        if row[\"species\"] in [\"Coffea arabica\", \"Theobroma cacao\"]:\n",
    "            jitter_deg = jitter_deg_default\n",
    "        else:\n",
    "            jitter_deg = jitter_deg_default  # could broaden if wanted\n",
    "\n",
    "        for _ in range(n_per_point):\n",
    "            lon_j = row[\"lon\"] + np.random.uniform(-jitter_deg, jitter_deg)\n",
    "            lat_j = row[\"lat\"] + np.random.uniform(-jitter_deg, jitter_deg)\n",
    "            jiggle_list.append({\n",
    "                \"species\": row[\"species\"],\n",
    "                \"lon\": lon_j,\n",
    "                \"lat\": lat_j,\n",
    "                \"source\": \"jiggle\"\n",
    "            })\n",
    "    return pd.DataFrame(jiggle_list)\n",
    "\n",
    "# Generate jiggles for all expert species\n",
    "jiggles = generate_jiggle_points(expert_occ, n_per_point=5, jitter_deg_default=0.1)\n",
    "\n",
    "# ====================================================\n",
    "# Final training set\n",
    "# ====================================================\n",
    "occ_all = pd.concat([occ_all, expert_occ, jiggles], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Final training set: {len(occ_all)} points across {occ_all['species'].nunique()} species\")\n",
    "occ_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cfcc5-6e8c-4405-9470-a5256bc0b763",
   "metadata": {},
   "source": [
    "### Building the training dataset for suitability modelling\n",
    "\n",
    "In this step, we prepare the **input dataset** that will be used to train the suitability model for each target species.\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1. **Load climate predictors**\n",
    "   - Open the `bio_stack` from TerraClimate (1990‚Äì2014 baseline).\n",
    "   - This contains gridded bioclimatic variables such as temperature, precipitation, PET, AET, and solar radiation.\n",
    "   - The file is an `xarray.Dataset`, which allows fast spatial extraction.\n",
    "\n",
    "2. **Define modelling extent**\n",
    "   - Specify the bounding box (`bbox`) for the study region.\n",
    "   - This ensures we only extract presence and background points from within the relevant spatial domain.\n",
    "\n",
    "3. **Select predictors**\n",
    "   - Choose which climate variables to use for model training.\n",
    "   - Selected variables can be adjusted depending on the research question and collinearity checks.\n",
    "\n",
    "4. **Build the training dataset**  \n",
    "   - Call `build_training_dataset()`:\n",
    "     - Filters the presence dataset (`occ_all`) to the study area.\n",
    "     - Generates background (pseudo‚Äëabsence) points within the study area.\n",
    "     - Extracts climate predictor values for all points from `bio_stack`.\n",
    "     - Combines presence (label‚ÄØ=‚ÄØ1) and background (label‚ÄØ=‚ÄØ0) into a single DataFrame.\n",
    "\n",
    "5. **Result**\n",
    "   - The resulting `data_all` contains:\n",
    "     - Geographic coordinates.\n",
    "     - Selected climate predictor values.\n",
    "     - Presence/absence labels.\n",
    "     - Species identifiers.\n",
    "   - This is the core dataset for training species distribution models.\n",
    "\n",
    "**Notes:**\n",
    "- The `background_ratio` parameter controls how many background points are generated relative to the number of presences (here: 3√ó).\n",
    "- This step **does not yet fit the model** ‚Äî it only prepares the inputs.\n",
    "- The presence dataset (`occ_all`) should be cleaned before this step (e.g., remove duplicates, isolates, and apply spatial thinning if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956c21b-b8c8-412d-ac67-b6fe2291186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 18395 presence points in study area.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szelie/python_projects/biofincas_climate_risk/utils_suitability_modelling.py:75: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n",
      "  land = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Querying predictors for 46100 points‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "from utils_suitability_modelling import build_training_dataset\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "# üìÇ Path to your climate predictor stack\n",
    "bio_path = DATA_DIR / \"terra_climate/SuitabilityVariables_1990_2014.nc\"\n",
    "\n",
    "# 1Ô∏è‚É£ Open dataset\n",
    "bio_stack = xr.open_dataset(bio_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üìã Inputs\n",
    "bbox = (-115, -50, 10, 25)  # adjust to your study area\n",
    "\n",
    "selected_predictors = [\n",
    "    \"MeanDiurnalRange\",\n",
    "    \"Isothermality\",\n",
    "    \"PrecSeasonality\",\n",
    "    \"AnnualPET\",\n",
    "    \"MeanTempDriestQuarter\",\n",
    "    \"PrecDriestMonth\",\n",
    "    \"PrecWettestMonth\",\n",
    "    \"AnnualAET\",\n",
    "    \"AnnualDeficit\",\n",
    "    \"MeanSRAD\",\n",
    "    \"MeanTempWettestQuarter\",\n",
    "    \"PrecWarmestQuarter\",\n",
    "    \"PrecColdestQuarter\",\n",
    "]\n",
    "#bio_stack = bio_stack.rename({\"lon\": \"y\", \"lat\": \"x\"})\n",
    "\n",
    "# üìÇ Run\n",
    "data_all = build_training_dataset(\n",
    "    presence_df=occ_all,       # df with ['species', 'lon', 'lat']\n",
    "    bio_stack=bio_stack,       # xarray.Dataset of your climate variables\n",
    "    bbox=bbox,\n",
    "    selected_predictors=selected_predictors,\n",
    "    background_ratio=3\n",
    ")\n",
    "\n",
    "data_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1c7e3-194b-46b9-96c9-14c5f286cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lon_min, lon_max, lat_min, lat_max = bbox\n",
    "\n",
    "# üìã Separate presence & absence\n",
    "presences = data_all[data_all[\"presence\"] == 1]\n",
    "absences  = data_all[data_all[\"presence\"] == 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Background points (absence)\n",
    "plt.scatter(\n",
    "    absences[\"lon\"], absences[\"lat\"],\n",
    "    color=\"grey\", alpha=0.5, s=10, label=\"Absence (background)\"\n",
    ")\n",
    "\n",
    "# Presence points\n",
    "plt.scatter(\n",
    "    presences[\"lon\"], presences[\"lat\"],\n",
    "    color=\"green\", alpha=0.9, s=20, marker='^', label=\"Presence\"\n",
    ")\n",
    "\n",
    "# Study area bbox\n",
    "plt.plot(\n",
    "    [lon_min, lon_max, lon_max, lon_min, lon_min],\n",
    "    [lat_min, lat_min, lat_max, lat_max, lat_min],\n",
    "    color=\"black\", linestyle=\"--\", linewidth=1, label=\"Study area\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend()\n",
    "plt.xlim(lon_min - 1, lon_max + 1)\n",
    "plt.ylim(lat_min - 1, lat_max + 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b3f6a-5d45-48cb-bae9-e77770f7a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Dominican Republic bounding box ---\n",
    "dr_bbox = (-72.1, -68.2, 17.4, 19.9)  # (min_lon, max_lon, min_lat, max_lat)\n",
    "lon_min, lon_max, lat_min, lat_max = dr_bbox\n",
    "\n",
    "# --- Filter for coffee presences and background absences in DR ---\n",
    "dr_points = data_all[\n",
    "    ((data_all[\"species\"] == \"Theobroma cacao\") | (data_all[\"species\"] == \"background\")) &\n",
    "    (data_all[\"lon\"] >= lon_min) & (data_all[\"lon\"] <= lon_max) &\n",
    "    (data_all[\"lat\"] >= lat_min) & (data_all[\"lat\"] <= lat_max)\n",
    "]\n",
    "\n",
    "# --- Separate presence & absence ---\n",
    "presences = dr_points[dr_points[\"presence\"] == 1]\n",
    "absences  = dr_points[dr_points[\"presence\"] == 0]\n",
    "\n",
    "# --- Load world borders ---\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Country borders\n",
    "world.boundary.plot(ax=ax, color=\"black\", linewidth=0.8)\n",
    "\n",
    "# Absence points\n",
    "ax.scatter(\n",
    "    absences[\"lon\"], absences[\"lat\"],\n",
    "    color=\"grey\", alpha=0.5, s=10, label=\"Absence (background)\"\n",
    ")\n",
    "\n",
    "# Presence points\n",
    "ax.scatter(\n",
    "    presences[\"lon\"], presences[\"lat\"],\n",
    "    color=\"green\", alpha=0.9, s=20, marker='^', label=\"Presence\"\n",
    ")\n",
    "\n",
    "# DR bbox outline\n",
    "ax.plot(\n",
    "    [lon_min, lon_max, lon_max, lon_min, lon_min],\n",
    "    [lat_min, lat_min, lat_max, lat_max, lat_min],\n",
    "    color=\"black\", linestyle=\"--\", linewidth=1, label=\"DR bbox\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Presence & Absence points for Coffea arabica ‚Äî Dominican Republic\")\n",
    "ax.legend()\n",
    "ax.set_xlim(lon_min - 0.5, lon_max + 0.5)\n",
    "ax.set_ylim(lat_min - 0.5, lat_max + 0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23064cd-4cde-4927-862b-7f7eb4814078",
   "metadata": {},
   "source": [
    "## --> our points are quite sparce for the DR, we would need more observations ideally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a5cdc-9aec-44c4-9fc5-422170aff311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eadd05a0-8645-40ee-91c3-ddf5bee08c1b",
   "metadata": {},
   "source": [
    "## Species Suitability Modelling with Random Forest\n",
    "\n",
    "The following cell trains **Random Forest** models to predict habitat suitability for multiple species based on environmental predictors.  \n",
    "It uses **presence/background points**, applies filtering to remove correlated and redundant predictors, and evaluates model performance.\n",
    "\n",
    "### Workflow\n",
    "1. **Input Data**\n",
    "   - `data_all` must contain:\n",
    "     - `species` ‚Äî species name\n",
    "     - `presence` ‚Äî binary presence/background label (`1` = presence, `0` = background)\n",
    "     - `lon`, `lat` ‚Äî coordinates in decimal degrees\n",
    "     - Environmental predictors matching `selected_predictors`\n",
    "\n",
    "2. **Settings**\n",
    "   - `species_to_plot` ‚Äî species for which to plot feature importance\n",
    "   - `min_distance_deg` ‚Äî minimum distance between presence and background points\n",
    "   - `corr_threshold` ‚Äî maximum Pearson correlation allowed\n",
    "   - `vif_threshold` ‚Äî maximum Variance Inflation Factor allowed\n",
    "   - `n_estimators` ‚Äî number of trees in the Random Forest\n",
    "   - `min_importance` ‚Äî minimum RF importance to retain predictors\n",
    "\n",
    "3. **Feature Selection**\n",
    "   - **Correlation filter**: remove highly correlated predictors\n",
    "   - **VIF filter**: remove predictors with high multicollinearity\n",
    "   - **Low-importance filter**: remove features with low RF importance\n",
    "\n",
    "4. **Model Training**\n",
    "   - Filter background points to be at least `min_distance_deg` from presence points\n",
    "   - Split into train/test sets (70/30, stratified)\n",
    "   - Train Random Forest\n",
    "   - Retrain after removing low-importance predictors\n",
    "\n",
    "5. **Evaluation**\n",
    "   - Compute **AUC** (Area Under ROC Curve)\n",
    "   - Determine optimal threshold for **max sensitivity + specificity**\n",
    "   - Plot feature importance for selected species\n",
    "\n",
    "6. **Outputs**\n",
    "   - AUC scores per species\n",
    "   - Feature importance plots\n",
    "   - Suitability predictions for each point\n",
    "   - Dictionaries of:\n",
    "     - `trained_classifiers`\n",
    "     - `feature_columns` used per species\n",
    "     - `results_by_species`\n",
    "   - `df_all_results` with predictions for all species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0119f7a-9039-4a17-aeec-2b12ed6539a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import requests\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# =========================================\n",
    "# SETTINGS\n",
    "# =========================================\n",
    "species_to_plot = [\"Coffea arabica\", \"Theobroma cacao\"]\n",
    "min_distance_deg = 0.1\n",
    "corr_threshold = 0.85\n",
    "vif_threshold = 10\n",
    "n_estimators = 300\n",
    "min_importance = 0.01\n",
    "\n",
    "selected_predictors = [\n",
    "    \"MeanDiurnalRange\", \"Isothermality\", \"PrecSeasonality\", \"AnnualPET\",\n",
    "    \"MeanTempDriestQuarter\", \"PrecDriestMonth\", \"PrecWettestMonth\", \"AnnualAET\",\n",
    "    \"AnnualDeficit\", \"MeanSRAD\", \"MeanTempWettestQuarter\",\n",
    "    \"PrecWarmestQuarter\", \"PrecColdestQuarter\"\n",
    "]\n",
    "\n",
    "# Expert base points (from you)\n",
    "coffee_points = [\n",
    "    Point(-71.243, 19.271),\n",
    "    Point(-71.778, 19.404),\n",
    "    Point(-70.981, 19.237),\n",
    "    Point(-70.960, 19.670),\n",
    "]\n",
    "cacao_points = [\n",
    "    Point(-71.125, 19.505),\n",
    "    Point(-71.228, 19.602),\n",
    "    Point(-71.606, 19.337),\n",
    "]\n",
    "\n",
    "EXPERT_POINTS = {\n",
    "    \"Coffea arabica\": coffee_points,\n",
    "    \"Theobroma cacao\": cacao_points,\n",
    "}\n",
    "\n",
    "EXPECTED_ALT = {\n",
    "    \"Coffea arabica\": (600, 2000),\n",
    "    \"Theobroma cacao\": (0, 800),\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# UTILS\n",
    "# =========================================\n",
    "def get_elevation(lat, lon):\n",
    "    \"\"\"Fetch elevation for one coordinate from Open-Elevation API.\"\"\"\n",
    "    url = \"https://api.open-elevation.com/api/v1/lookup\"\n",
    "    params = {\"locations\": f\"{lat},{lon}\"}\n",
    "    r = requests.get(url, params=params, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"results\"][0][\"elevation\"]\n",
    "\n",
    "def jitter_points(points, n_extra=3, jitter_deg=0.05):\n",
    "    \"\"\"Create extra points around expert seeds with random jitter.\"\"\"\n",
    "    out = []\n",
    "    for p in points:\n",
    "        out.append(p)  # keep original\n",
    "        for _ in range(n_extra):\n",
    "            lon = p.x + np.random.uniform(-jitter_deg, jitter_deg)\n",
    "            lat = p.y + np.random.uniform(-jitter_deg, jitter_deg)\n",
    "            out.append(Point(lon, lat))\n",
    "    return out\n",
    "\n",
    "def drop_correlated(df, predictors, threshold=0.85):\n",
    "    corr = df[predictors].corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    keep = [p for p in predictors if p not in to_drop]\n",
    "    return keep, to_drop\n",
    "\n",
    "def calculate_vif(df, predictors):\n",
    "    X = df[predictors].dropna()\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = predictors\n",
    "    vif_data[\"VIF\"] = [sm.OLS(X[col], sm.add_constant(X.drop(columns=[col]))).fit().rsquared for col in X.columns]\n",
    "    vif_data[\"VIF\"] = 1 / (1 - vif_data[\"VIF\"])\n",
    "    return vif_data\n",
    "\n",
    "def drop_high_vif(df, predictors, threshold=10):\n",
    "    vif_data = calculate_vif(df, predictors)\n",
    "    to_drop = vif_data.loc[vif_data[\"VIF\"] > threshold, \"feature\"].tolist()\n",
    "    keep = [p for p in predictors if p not in to_drop]\n",
    "    return keep, to_drop, vif_data\n",
    "\n",
    "def max_sens_spec_threshold(y_true, y_prob):\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    best_thresh = 0.5\n",
    "    best_score = -1\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sens = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        spec = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "        score = sens + spec\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thr\n",
    "    return best_thresh\n",
    "\n",
    "# =========================================\n",
    "# TRAINING LOOP\n",
    "# =========================================\n",
    "auc_results = {}\n",
    "results_by_species = {}\n",
    "trained_classifiers = {}\n",
    "feature_columns = {}\n",
    "\n",
    "for species in data_all[\"species\"].unique():\n",
    "    if species == \"background\":\n",
    "        continue\n",
    "\n",
    "    df_presence = data_all[(data_all[\"species\"] == species) & (data_all[\"presence\"] == 1)].copy()\n",
    "    df_background_all = data_all[data_all[\"presence\"] == 0].copy()\n",
    "\n",
    "    if len(df_presence) < 10:\n",
    "        continue\n",
    "\n",
    "    # print(f\"\\nüå± Training model for {species} ({len(df_presence)} presences)\")\n",
    "\n",
    "    # # --- Inject jittered expert presences ---\n",
    "    # if species in EXPERT_POINTS:\n",
    "    #     jittered = jitter_points(EXPERT_POINTS[species], n_extra=20, jitter_deg=0.5)\n",
    "    #     min_alt, max_alt = EXPECTED_ALT[species]\n",
    "    #     injected = []\n",
    "    #     for pt in jittered:\n",
    "    #         try:\n",
    "    #             elev = get_elevation(pt.y, pt.x)\n",
    "    #             if elev is not None and min_alt <= elev <= max_alt:\n",
    "    #                 df_point = data_all.loc[\n",
    "    #                     (np.isclose(data_all[\"lat\"], pt.y, atol=0.01)) &\n",
    "    #                     (np.isclose(data_all[\"lon\"], pt.x, atol=0.01))\n",
    "    #                 ].copy()\n",
    "    #                 if not df_point.empty:\n",
    "    #                     df_point = df_point.iloc[[0]].copy()\n",
    "    #                     df_point[\"species\"] = species\n",
    "    #                     df_point[\"presence\"] = 1\n",
    "    #                     df_point[\"elevation\"] = elev\n",
    "    #                     injected.append(df_point)\n",
    "    #                     print(f\"‚ûï Added expert presence for {species} at {pt} (elev={elev} m)\")\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"‚ö†Ô∏è Could not fetch elevation for {species} at {pt}: {e}\")\n",
    "    #     if injected:\n",
    "    #         df_presence = pd.concat([df_presence] + injected, ignore_index=True)\n",
    "\n",
    "    # --- Background filtering ---\n",
    "    pres_coords = df_presence[[\"lon\", \"lat\"]].to_numpy()\n",
    "    tree = cKDTree(pres_coords)\n",
    "    bg_far = []\n",
    "    for _, row in df_background_all.iterrows():\n",
    "        dist, _ = tree.query([row[\"lon\"], row[\"lat\"]], k=1)\n",
    "        if dist >= min_distance_deg:\n",
    "            bg_far.append(row)\n",
    "    df_background = pd.DataFrame(bg_far)\n",
    "\n",
    "    # --- Combine data ---\n",
    "    df = pd.concat([df_presence, df_background], ignore_index=True)\n",
    "\n",
    "    # --- Correlation filtering ---\n",
    "    predictors_filtered, dropped_corr = drop_correlated(df, selected_predictors, corr_threshold)\n",
    "    if dropped_corr:\n",
    "        print(f\"üìâ Dropped correlated predictors: {dropped_corr}\")\n",
    "\n",
    "    # --- VIF filtering ---\n",
    "    predictors_filtered, dropped_vif, vif_df = drop_high_vif(df, predictors_filtered, vif_threshold)\n",
    "    if dropped_vif:\n",
    "        print(f\"üìâ Dropped high-VIF predictors: {dropped_vif}\")\n",
    "\n",
    "    X = df[predictors_filtered]\n",
    "    y = df[\"presence\"]\n",
    "\n",
    "    # --- Train/Test split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Train RF ---\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # --- Drop low-importance features and retrain ---\n",
    "    importances = pd.Series(clf.feature_importances_, index=predictors_filtered)\n",
    "    low_imp = importances[importances < min_importance].index.tolist()\n",
    "    if low_imp:\n",
    "        print(f\"üìâ Dropping low-importance features: {low_imp}\")\n",
    "        predictors_filtered = [p for p in predictors_filtered if p not in low_imp]\n",
    "        X_train = X_train[predictors_filtered]\n",
    "        X_test = X_test[predictors_filtered]\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "    # --- Store classifier ---\n",
    "    trained_classifiers[species] = clf\n",
    "    feature_columns[species] = predictors_filtered\n",
    "\n",
    "    # --- AUC on holdout ---\n",
    "    y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    auc_results[species] = auc\n",
    "    print(f\"üå≥ AUC (holdout): {auc:.3f}\")\n",
    "\n",
    "    # --- Threshold ---\n",
    "    threshold = max_sens_spec_threshold(y_test, y_pred_prob)\n",
    "    print(f\"üéØ Threshold (max sens+spec): {threshold:.2f}\")\n",
    "\n",
    "    # --- Cross-validation AUC ---\n",
    "    cv_auc = []\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_tr, X_te = X.iloc[train_idx][predictors_filtered], X.iloc[test_idx][predictors_filtered]\n",
    "        y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model_cv = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "        model_cv.fit(X_tr, y_tr)\n",
    "        prob_te = model_cv.predict_proba(X_te)[:, 1]\n",
    "        cv_auc.append(roc_auc_score(y_te, prob_te))\n",
    "\n",
    "    print(f\"üîÅ Cross-val AUC: {np.mean(cv_auc):.3f} ¬± {np.std(cv_auc):.3f}\")\n",
    "\n",
    "    # --- Permutation importance ---\n",
    "    perm = permutation_importance(clf, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "    sorted_idx = perm.importances_mean.argsort()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(np.array(predictors_filtered)[sorted_idx], perm.importances_mean[sorted_idx])\n",
    "    plt.xlabel(\"Mean Permutation Importance\")\n",
    "    plt.title(f\"Permutation Importance ‚Äî {species}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Save predictions ---\n",
    "    df[\"suitability\"] = clf.predict_proba(X[predictors_filtered])[:, 1]\n",
    "    df[\"predicted_presence\"] = (df[\"suitability\"] >= threshold).astype(int)\n",
    "    results_by_species[species] = df\n",
    "\n",
    "    # --- Feature importance plot ---\n",
    "    if species in species_to_plot:\n",
    "        importances = pd.Series(clf.feature_importances_, index=predictors_filtered).sort_values()\n",
    "        importances.plot.barh()\n",
    "        plt.title(f\"Feature Importance ‚Äî {species}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# =========================================\n",
    "# SUMMARY\n",
    "# =========================================\n",
    "print(\"\\nüéâ AUC summary:\")\n",
    "for k, v in auc_results.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "df_all_results = pd.concat(results_by_species.values(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cec8e-5f5f-4f53-a3c3-6ebe8bedc3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70580a-b8c8-41fa-ad01-1a8f053747af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e26481-6295-4f93-bb4a-e4a1cb683360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924be16-6fae-4f0d-894b-f70b1a83db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we addded a couple of points because the region of Centro Naturaleza was not covered with observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3295cd-8564-4c1c-a5de-d872825c32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "species = \"Theobroma cacao\"\n",
    "df_obs = results_by_species[species]\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "# background points\n",
    "plt.scatter(df_obs.loc[df_obs[\"presence\"]==0, \"lon\"],\n",
    "            df_obs.loc[df_obs[\"presence\"]==0, \"lat\"],\n",
    "            c=\"lightgrey\", s=10, alpha=0.5, label=\"Background\")\n",
    "\n",
    "# presence points\n",
    "plt.scatter(df_obs.loc[df_obs[\"presence\"]==1, \"lon\"],\n",
    "            df_obs.loc[df_obs[\"presence\"]==1, \"lat\"],\n",
    "            c=\"darkgreen\", s=30, label=\"Presence\")\n",
    "\n",
    "# predicted suitable\n",
    "plt.scatter(df_obs.loc[df_obs[\"predicted_presence\"]==1, \"lon\"],\n",
    "            df_obs.loc[df_obs[\"predicted_presence\"]==1, \"lat\"],\n",
    "            facecolors=\"none\", edgecolors=\"red\", s=40, label=\"Predicted suitable\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend()\n",
    "plt.title(f\"Observations and Predictions ‚Äî {species}\")\n",
    "\n",
    "# üåç Zoom in to DR\n",
    "plt.xlim(-72.2, -68.3)\n",
    "plt.ylim(17.4, 20.1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1254c22-bf65-443b-ad53-cd4217aa565d",
   "metadata": {},
   "source": [
    "## Predicting Suitability Maps for Current and Future Climate\n",
    "\n",
    "This cell uses the **trained Random Forest models** to predict species suitability  \n",
    "across the study area for **current** and **future** climate conditions.\n",
    "\n",
    "### Steps\n",
    "1. **Prepare data**\n",
    "   - Use `df_now` (current climate) and `df_future` (future climate).\n",
    "   - Drop rows with missing predictor values for each species‚Äô selected features.\n",
    "\n",
    "2. **Generate predictions**\n",
    "   - For each trained classifier (`trained_classifiers`):\n",
    "     - Extract the relevant predictors (`feature_columns[species]`).\n",
    "     - Predict suitability **for each grid cell**.\n",
    "\n",
    "3. **Estimate prediction uncertainty**\n",
    "   - Run predictions for each individual decision tree in the Random Forest.\n",
    "   - Compute:\n",
    "     - **Mean suitability** (`suitability`)\n",
    "     - **Standard deviation** across trees (`suitability_std`)\n",
    "\n",
    "4. **Save results**\n",
    "   - Store both mean and std for:\n",
    "     - `suitability_maps_now[species]` ‚Äî current climate\n",
    "     - `suitability_maps_future[species]` ‚Äî future climate\n",
    "   - Each entry contains:\n",
    "     - `lon`, `lat`\n",
    "     - Mean suitability score\n",
    "     - Prediction uncertainty (std)\n",
    "\n",
    "### Output\n",
    "- `suitability_maps_now`: dict of DataFrames with suitability & uncertainty for each species (current climate).\n",
    "- `suitability_maps_future`: same as above for future climate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43da99-8c98-4604-8c52-eb8b7a9a7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def load_and_crop_climate_data(nc_path, bbox):\n",
    "    \"\"\"\n",
    "    Load a NetCDF climate dataset, ensure lat/lon ordering,\n",
    "    crop to bounding box, and return as a DataFrame without NaNs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nc_path : str\n",
    "        Path to the NetCDF file.\n",
    "    bbox : tuple\n",
    "        (min_lon, max_lon, min_lat, max_lat) bounding box.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Climate data for the selected bounding box.\n",
    "    \"\"\"\n",
    "    # üìÇ Load dataset\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "\n",
    "    # üîÑ Ensure lat ascending\n",
    "    if ds.lat.values[0] > ds.lat.values[-1]:\n",
    "        ds = ds.reindex(lat=list(reversed(ds.lat)))\n",
    "\n",
    "    # üîÑ Ensure lon in -180..180 range\n",
    "    if ds.lon.max() > 180:\n",
    "        ds = ds.assign_coords(lon=((ds.lon + 180) % 360) - 180)\n",
    "    ds = ds.sortby(\"lon\")\n",
    "\n",
    "    # ‚úÇÔ∏è Crop dataset to bbox\n",
    "    lat_min, lat_max = sorted([bbox[2], bbox[3]])\n",
    "    lon_min, lon_max = sorted([bbox[0], bbox[1]])\n",
    "    ds_crop = ds.sel(lon=slice(lon_min, lon_max), lat=slice(lat_min, lat_max))\n",
    "\n",
    "    # üöø Drop all-NaN grid cells\n",
    "    ds_crop = ds_crop.dropna(dim=\"lat\", how=\"all\").dropna(dim=\"lon\", how=\"all\")\n",
    "\n",
    "    # üìä Convert to DataFrame\n",
    "    df = ds_crop.to_dataframe().reset_index().dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# üìç Define bounding box\n",
    "bbox = (-100.98, -68.0, 13.0, 21.98)\n",
    "\n",
    "# üìÑ Load current climate\n",
    "df_now = load_and_crop_climate_data(\n",
    "    \"/Users/szelie/data/unu/terra_climate/SuitabilityVariables_1990_2014.nc\",\n",
    "    bbox\n",
    ")\n",
    "\n",
    "# üìÑ Load future climate\n",
    "df_future = load_and_crop_climate_data(\n",
    "    \"/Users/szelie/data/unu/terra_climate_scenarios_ncss/plus2C/SuitabilityVariables_plus2C_1990_2014.nc\",\n",
    "    bbox\n",
    ")\n",
    "\n",
    "# üìÑ Load future climate\n",
    "df_4c = load_and_crop_climate_data(\n",
    "    \"/Users/szelie/data/unu/terra_climate_scenarios_ncss/plus4C/SuitabilityVariables_plus4C_1990_2014.nc\",\n",
    "    bbox\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ df_now: {len(df_now)} grid points, df_future: {len(df_future)} grid points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba2179-cf41-4746-8854-17f79f47e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01198f-f3d5-48c5-b2fd-853f90da169f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suitability_maps_now = {}\n",
    "suitability_maps_future = {}\n",
    "suitability_maps_4c = {}  # 4¬∞C scenario\n",
    "\n",
    "for species in trained_classifiers:\n",
    "    clf = trained_classifiers[species]\n",
    "    features = feature_columns[species]\n",
    "\n",
    "    # Drop NaNs\n",
    "    df_now_valid    = df_now.dropna(subset=features).copy()\n",
    "    df_future_valid = df_future.dropna(subset=features).copy()\n",
    "    df_4c_valid     = df_4c.dropna(subset=features).copy()\n",
    "\n",
    "    # Prepare design matrices as NumPy (avoids the warning)\n",
    "    X_now    = df_now_valid[features].to_numpy()\n",
    "    X_future = df_future_valid[features].to_numpy()\n",
    "    X_4c     = df_4c_valid[features].to_numpy()\n",
    "\n",
    "    # NOW\n",
    "    tree_preds_now = np.stack([tree.predict_proba(X_now)[:, 1] for tree in clf.estimators_], axis=1)\n",
    "    df_now_valid[\"suitability\"] = tree_preds_now.mean(axis=1)\n",
    "    df_now_valid[\"suitability_std\"] = tree_preds_now.std(axis=1)\n",
    "    suitability_maps_now[species] = df_now_valid[[\"lon\", \"lat\", \"suitability\", \"suitability_std\"]]\n",
    "\n",
    "    # FUTURE (2¬∞C)\n",
    "    tree_preds_future = np.stack([tree.predict_proba(X_future)[:, 1] for tree in clf.estimators_], axis=1)\n",
    "    df_future_valid[\"suitability\"] = tree_preds_future.mean(axis=1)\n",
    "    df_future_valid[\"suitability_std\"] = tree_preds_future.std(axis=1)\n",
    "    suitability_maps_future[species] = df_future_valid[[\"lon\", \"lat\", \"suitability\", \"suitability_std\"]]\n",
    "\n",
    "    # 4¬∞C\n",
    "    tree_preds_4c = np.stack([tree.predict_proba(X_4c)[:, 1] for tree in clf.estimators_], axis=1)\n",
    "    df_4c_valid[\"suitability\"] = tree_preds_4c.mean(axis=1)\n",
    "    df_4c_valid[\"suitability_std\"] = tree_preds_4c.std(axis=1)\n",
    "    suitability_maps_4c[species] = df_4c_valid[[\"lon\", \"lat\", \"suitability\", \"suitability_std\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27b45c-dc31-4a22-b950-0474779116d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564437d5-76f3-471e-9aac-cbab21b75d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_dir = str(DATA_DIR / \"suitability\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "def save_suitability_dict(suit_dict, label):\n",
    "    for species, df in suit_dict.items():\n",
    "        fname = f\"{label}_{species.replace(' ', '_')}.parquet\"\n",
    "        df.to_parquet(os.path.join(out_dir, fname), index=False)\n",
    "\n",
    "save_suitability_dict(suitability_maps_now, \"now\")\n",
    "save_suitability_dict(suitability_maps_future, \"future\")\n",
    "save_suitability_dict(suitability_maps_4c, \"4c\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b821d2-72f8-411d-8b77-47c613690e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Species ---\n",
    "species = 'Persea americana'\n",
    "\n",
    "# --- Threshold from prevalence ---\n",
    "coffee_threshold = results_by_species[species][\"predicted_presence\"].mean()\n",
    "\n",
    "# --- Get suitability for NOW, 2¬∞C, and 4¬∞C ---\n",
    "df_now = suitability_maps_now[species].copy()\n",
    "df_2c = suitability_maps_future[species].copy()\n",
    "df_4c = suitability_maps_4c[species].copy()\n",
    "\n",
    "# --- Rename columns ---\n",
    "df_now = df_now.rename(columns={\"suitability\": \"suit_now\"})\n",
    "df_2c = df_2c.rename(columns={\"suitability\": \"suit_2c\"})\n",
    "df_4c = df_4c.rename(columns={\"suitability\": \"suit_4c\"})\n",
    "\n",
    "# --- Merge with NOW ---\n",
    "merge_2c = df_now.merge(df_2c[[\"lon\", \"lat\", \"suit_2c\"]], on=[\"lon\", \"lat\"], how=\"inner\")\n",
    "merge_4c = df_now.merge(df_4c[[\"lon\", \"lat\", \"suit_4c\"]], on=[\"lon\", \"lat\"], how=\"inner\")\n",
    "\n",
    "# --- Classify points ---\n",
    "def classify_suitability(row, col_future):\n",
    "    if row[\"suit_now\"] >= coffee_threshold and row[col_future] >= coffee_threshold:\n",
    "        return \"Suitable both\"\n",
    "    elif row[\"suit_now\"] < coffee_threshold and row[col_future] >= coffee_threshold:\n",
    "        return \"Newly suitable\"\n",
    "    elif row[\"suit_now\"] >= coffee_threshold and row[col_future] < coffee_threshold:\n",
    "        return \"No longer suitable\"\n",
    "    else:\n",
    "        return \"Unsuitable both\"\n",
    "\n",
    "merge_2c[\"category\"] = merge_2c.apply(lambda row: classify_suitability(row, \"suit_2c\"), axis=1)\n",
    "merge_4c[\"category\"] = merge_4c.apply(lambda row: classify_suitability(row, \"suit_4c\"), axis=1)\n",
    "\n",
    "# --- Convert to GeoDataFrames ---\n",
    "merge_2c[\"geometry\"] = [Point(xy) for xy in zip(merge_2c[\"lon\"], merge_2c[\"lat\"])]\n",
    "merge_4c[\"geometry\"] = [Point(xy) for xy in zip(merge_4c[\"lon\"], merge_4c[\"lat\"])]\n",
    "\n",
    "gdf_2c = gpd.GeoDataFrame(merge_2c, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "gdf_4c = gpd.GeoDataFrame(merge_4c, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Colors ---\n",
    "category_colors = {\n",
    "    \"Unsuitable both\": \"lightgrey\",\n",
    "    \"Newly suitable\": \"blue\",\n",
    "    \"No longer suitable\": \"red\",\n",
    "    \"Suitable both\": \"green\",\n",
    "}\n",
    "\n",
    "# --- Create figure with two maps ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True, sharey=True)\n",
    "\n",
    "for ax, gdf, title in zip(\n",
    "    axs,\n",
    "    [gdf_2c, gdf_4c],\n",
    "    [f\"{species}: 2¬∞C scenario\", f\"{species}: 4¬∞C scenario\"]\n",
    "):\n",
    "    for category, color in category_colors.items():\n",
    "        subset = gdf[gdf[\"category\"] == category]\n",
    "        if not subset.empty:\n",
    "            subset.plot(ax=ax, color=color, markersize=1, alpha=0.7)\n",
    "        else:\n",
    "            ax.plot([], [], color=color)  # Dummy for legend\n",
    "\n",
    "    ax.set_title(f\"{title}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# --- Custom legend (same for both) ---\n",
    "legend_handles = [\n",
    "    mlines.Line2D([], [], color=color, marker='o', linestyle='None',\n",
    "                  markersize=5, label=category)\n",
    "    for category, color in category_colors.items()\n",
    "]\n",
    "axs[1].legend(handles=legend_handles, title=\"Category\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2d352-bedb-4ded-b3ec-3ebadcaefde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Species ---\n",
    "species = \"Coffea arabica\"\n",
    "\n",
    "# --- Threshold from prevalence ---\n",
    "coffee_threshold = results_by_species[species][\"predicted_presence\"].mean()\n",
    "\n",
    "# --- Get suitability for NOW, 2¬∞C, and 4¬∞C ---\n",
    "df_now = suitability_maps_now[species].copy()\n",
    "df_2c = suitability_maps_future[species].copy()\n",
    "df_4c = suitability_maps_4c[species].copy()\n",
    "\n",
    "# --- Rename columns ---\n",
    "df_now = df_now.rename(columns={\"suitability\": \"suit_now\"})\n",
    "df_2c = df_2c.rename(columns={\"suitability\": \"suit_2c\"})\n",
    "df_4c = df_4c.rename(columns={\"suitability\": \"suit_4c\"})\n",
    "\n",
    "# --- Merge with NOW ---\n",
    "merge_2c = df_now.merge(df_2c[[\"lon\", \"lat\", \"suit_2c\"]], on=[\"lon\", \"lat\"], how=\"inner\")\n",
    "merge_4c = df_now.merge(df_4c[[\"lon\", \"lat\", \"suit_4c\"]], on=[\"lon\", \"lat\"], how=\"inner\")\n",
    "\n",
    "# --- Classify points ---\n",
    "def classify_suitability(row, col_future):\n",
    "    if row[\"suit_now\"] >= coffee_threshold and row[col_future] >= coffee_threshold:\n",
    "        return \"Suitable both\"\n",
    "    elif row[\"suit_now\"] < coffee_threshold and row[col_future] >= coffee_threshold:\n",
    "        return \"Newly suitable\"\n",
    "    elif row[\"suit_now\"] >= coffee_threshold and row[col_future] < coffee_threshold:\n",
    "        return \"No longer suitable\"\n",
    "    else:\n",
    "        return \"Unsuitable both\"\n",
    "\n",
    "merge_2c[\"category\"] = merge_2c.apply(lambda row: classify_suitability(row, \"suit_2c\"), axis=1)\n",
    "merge_4c[\"category\"] = merge_4c.apply(lambda row: classify_suitability(row, \"suit_4c\"), axis=1)\n",
    "\n",
    "# --- Convert to GeoDataFrames ---\n",
    "merge_2c[\"geometry\"] = [Point(xy) for xy in zip(merge_2c[\"lon\"], merge_2c[\"lat\"])]\n",
    "merge_4c[\"geometry\"] = [Point(xy) for xy in zip(merge_4c[\"lon\"], merge_4c[\"lat\"])]\n",
    "\n",
    "gdf_2c = gpd.GeoDataFrame(merge_2c, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "gdf_4c = gpd.GeoDataFrame(merge_4c, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Colors ---\n",
    "category_colors = {\n",
    "    \"Unsuitable both\": \"lightgrey\",\n",
    "    \"Newly suitable\": \"blue\",\n",
    "    \"No longer suitable\": \"red\",\n",
    "    \"Suitable both\": \"green\",\n",
    "}\n",
    "\n",
    "# --- Create figure with two maps ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True, sharey=True)\n",
    "\n",
    "for ax, gdf, title in zip(\n",
    "    axs,\n",
    "    [gdf_2c, gdf_4c],\n",
    "    [f\"{species}: 2¬∞C scenario\", f\"{species}: 4¬∞C scenario\"]\n",
    "):\n",
    "    for category, color in category_colors.items():\n",
    "        subset = gdf[gdf[\"category\"] == category]\n",
    "        if not subset.empty:\n",
    "            subset.plot(ax=ax, color=color, markersize=1, alpha=0.7)\n",
    "        else:\n",
    "            ax.plot([], [], color=color)  # Dummy for legend\n",
    "\n",
    "    ax.set_title(f\"{title}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# --- Custom legend (same for both) ---\n",
    "legend_handles = [\n",
    "    mlines.Line2D([], [], color=color, marker='o', linestyle='None',\n",
    "                  markersize=5, label=category)\n",
    "    for category, color in category_colors.items()\n",
    "]\n",
    "axs[1].legend(handles=legend_handles, title=\"Category\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c85c4b-e0a8-4d80-a0d4-c1a249675a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Species ---\n",
    "species = 'Theobroma cacao'\n",
    "\n",
    "# --- Threshold from prevalence ---\n",
    "coffee_threshold = results_by_species[species][\"predicted_presence\"].mean()\n",
    "\n",
    "# --- Get suitability for NOW, 2¬∞C, and 4¬∞C ---\n",
    "df_now = suitability_maps_now[species].copy()\n",
    "df_2c = suitability_maps_future[species].copy()\n",
    "df_4c = suitability_maps_4c[species].copy()\n",
    "\n",
    "# --- Rename columns ---\n",
    "df_now = df_now.rename(columns={\"suitability\": \"suit_now\"})\n",
    "df_2c = df_2c.rename(columns={\"suitability\": \"suit_2c\"})\n",
    "df_4c = df_4c.rename(columns={\"suitability\": \"suit_4c\"})\n",
    "\n",
    "# --- Merge with NOW ---\n",
    "merge_2c = df_now.merge(df_2c[[\"lon\", \"lat\", \"suit_2c\"]], on=[\"lon\", \"lat\"], how=\"inner\")\n",
    "merge_4c = df_now.merge(df_4c[[\"lon\", \"lat\", \"suit_4c\"]], on=[\"lon\", \"lat\"], how=\"inner\")\n",
    "\n",
    "# --- Classify points ---\n",
    "def classify_suitability(row, col_future):\n",
    "    if row[\"suit_now\"] >= coffee_threshold and row[col_future] >= coffee_threshold:\n",
    "        return \"Suitable both\"\n",
    "    elif row[\"suit_now\"] < coffee_threshold and row[col_future] >= coffee_threshold:\n",
    "        return \"Newly suitable\"\n",
    "    elif row[\"suit_now\"] >= coffee_threshold and row[col_future] < coffee_threshold:\n",
    "        return \"No longer suitable\"\n",
    "    else:\n",
    "        return \"Unsuitable both\"\n",
    "\n",
    "merge_2c[\"category\"] = merge_2c.apply(lambda row: classify_suitability(row, \"suit_2c\"), axis=1)\n",
    "merge_4c[\"category\"] = merge_4c.apply(lambda row: classify_suitability(row, \"suit_4c\"), axis=1)\n",
    "\n",
    "# --- Convert to GeoDataFrames ---\n",
    "merge_2c[\"geometry\"] = [Point(xy) for xy in zip(merge_2c[\"lon\"], merge_2c[\"lat\"])]\n",
    "merge_4c[\"geometry\"] = [Point(xy) for xy in zip(merge_4c[\"lon\"], merge_4c[\"lat\"])]\n",
    "\n",
    "gdf_2c = gpd.GeoDataFrame(merge_2c, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "gdf_4c = gpd.GeoDataFrame(merge_4c, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Colors ---\n",
    "category_colors = {\n",
    "    \"Unsuitable both\": \"lightgrey\",\n",
    "    \"Newly suitable\": \"blue\",\n",
    "    \"No longer suitable\": \"red\",\n",
    "    \"Suitable both\": \"green\",\n",
    "}\n",
    "\n",
    "# --- Create figure with two maps ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharex=True, sharey=True)\n",
    "\n",
    "for ax, gdf, title in zip(\n",
    "    axs,\n",
    "    [gdf_2c, gdf_4c],\n",
    "    [f\"{species}: 2¬∞C scenario\", f\"{species}: 4¬∞C scenario\"]\n",
    "):\n",
    "    for category, color in category_colors.items():\n",
    "        subset = gdf[gdf[\"category\"] == category]\n",
    "        if not subset.empty:\n",
    "            subset.plot(ax=ax, color=color, markersize=1, alpha=0.7)\n",
    "        else:\n",
    "            ax.plot([], [], color=color)  # Dummy for legend\n",
    "\n",
    "    ax.set_title(f\"{title}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# --- Custom legend (same for both) ---\n",
    "legend_handles = [\n",
    "    mlines.Line2D([], [], color=color, marker='o', linestyle='None',\n",
    "                  markersize=5, label=category)\n",
    "    for category, color in category_colors.items()\n",
    "]\n",
    "axs[1].legend(handles=legend_handles, title=\"Category\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04a007-bb82-4dac-a4e8-64f77a91156d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
